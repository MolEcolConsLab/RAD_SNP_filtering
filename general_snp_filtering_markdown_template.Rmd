---
title: "SNP FILTERING"
author: "Baron Stacks"
date: "4/27/2021"
  output:
  html_document:
    toc: yes
  html_notebook:
    code_folding: hide
    df_print: paged
    theme: yeti
    toc: yes
---

1) note to self: Added function for LD filtering to the filterSNPs_UPDATED script but did not add it here yet. Thinking we actually want to filter for LD using bcftools before importing to R.

## Install and load libraries
```{r load libraries, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
rm(list=ls())

# make sure you have Rtools installed
#if (!require("devtools")) install.packages("devtools")

# install latest versions of required packages from GitHub, etc. (NB need XCode command line tools installed for some of these)
#devtools::install_github("thierrygosselin/radiator")
#devtools::install_github('ericarcher/strataG', build_vignettes = TRUE)
#devtools::install_github("thibautjombart/adegenet")
#if (!requireNamespace("BiocManager", quietly=TRUE))
#    install.packages("BiocManager")
#BiocManager::install("SNPRelate")

library(vcfR)
library(tidyverse)
library(ggpubr)
library(RColorBrewer)
library(Polychrome)
library(HardyWeinberg)
library(strataG)
library(radiator)
library(SeqArray)
library(genetics)
library(whoa)

#Source custom functions (change paths accordingly if needed)
source("scripts/ggplot.R")
source("scripts/VCFfilterstats.R")
source("scripts/xtrafunctions.R")
source("scripts/plotstats.R")
source("scripts/filterSNPs_UPDATED.R")
source("scripts/HDplot.R")

options(dplyr.summarise.inform = FALSE) #Shut down notices for summarise (they're annoying)
```

**This script is split into 11 steps:**
1) Import stats from vcftools for full dataset and visualize using a script from O'Leary et. al.
2) Read in VCF and visualize paralogs wth HDPlot
3) Visualize genotype frequency and assess heterozygote miscall rate
4) Filter paralogs and convert vcf to tidy (long) format
5) Set parameters for filtering and target values for visualization
6) Filter individuals and loci with lots of missing data
7) Plot filtered data
8) Evaluate HWE, heterozygosity and short-distance LD
9) Filter for heterozygosity, neutral SNPs (ie HWE), and LD if desired
10) Re-calculate missingness and heterozygosity after filtering
11) Plot missingness metrics after all filtering
12) Export data

**How to navigate this document:**
The steps summarized above are split into chunks. An accompanying Gist document with example graphs and explanation is linked in the Github repository. A summary of what each chunk does, as well as any required input files, are just above each chunk and reiterated in that document. Most of the time, the required input files will just be the output of the previous chunks. Any parameters required in a chunk will be specified at the top.

Move through this document one section at a time, monitoring the output and setting the filters accordingly.

## Before starting
**Required input: **
The following folders/files should be present in your working directory. *The below code assumes you are running the chunks from your working directory so the relative file paths are accurate.*

1. a folder called "data" containing the following files, all of which should have been output by vcftools into the "09_vcftools_out" folder in the previous step:
  - a .vcf file *less than 1GB in size* containing all variants and samples output by vcftools. This will usually end with ".recode.vcf" **this is required** *(the recommended file size is so that this can be run locally on most laptop or desktop computers, and is reasonable for a SNP file lightly filtered from vcftools and with moderate sample sizes. Those with very large sample sizes and/or loci may be larger than this, and could be run on a computer with more capacity, or this markdown doc could be amended to be run as a script on a cluster, etc.-but we haven't tested it that way yet)*
  - a tab-delimited population map file, where the first column contains the sample names, and the second column specifies the (putative) population **this is required**
  - .het file with individual heterozygosity **not required**
  - .idepth file with depth info by individual **not required**
  - .imiss file with missingness info by individual **not required**
  - .ldepth file with depth info by locus **not required**
  - .lmiss file with missingness info by locus **not required**


**If you are missing any of the files listed above that are not required, then just skip lines 128-133; the rest will run as long as you have a a vcf file and population map**

2. a folder called **scripts** with the following scripts:
  - ggplot.R
  - VCFfilterstats.R
  - plotstats.R
  - filterSNPs_UPDATED.R


### Background information about vcf files
See this link for information about vcf file format: https://knausb.github.io/vcfR_documentation/vcf_data.html

In general, VCF files contain three regions: the meta region, the fixed region, and the gt region.
- The meta region contains information that helps understand and navigate the other regions
- The fixed region contains summary data about the variants e.g. position, reference, alternate, and quality
- The gt region contains genotype information from each individual





## Chunk 1: Visualize raw stats
Note that chunks 1-3 take awhile to run.

The dark blue line in each graph is the mean.
**BEFORE RUNNING:** 
1. Assign the path of the data folder to "file_directory", 
2. Assign the prefix for your vcf file (i.e. the part before "recode.vcf") to the object file_prefix,
3. Assign the prefix to the popmap file (i.e. the part before .txt) to the "popmap_file" object


```{r stats raw, fig.height=20, fig.width=10, message=FALSE, warning=FALSE}
#setwd("...") #(if desired, else comment out)
# load stats files - O'Leary ----
file_directory <- "../data/"
file_prefix <- "CNRLD3"
popmap_file <- "popmap_allCNR"

#Load stats files for viz
ind_stats_raw <- read.ind.stats(dir = file_directory, vcf = file_prefix) #Requires the output files from vcftools with the following extensions: .idepth, .imiss, .het
loc_stats_raw <- read.loc.stats(dir = file_directory, vcf = file_prefix) #Requires the output files from vcftools with the following extensions: .ldepth, .lmiss

#Plot raw stats
plot_stats_init(ind_stats_raw, loc_stats_raw)
```



## Chunk 2: Read in VCF and visualize paralogs

This chunk uses the program vcfR to read the population map and vcf file into R and "tidy" the dataframe (i.e. convert it to long format), which allows us to easily visualize and filter the data. The first step is to identify putative paralogs that have been collapsed together following the method outlined by McKinnery et. al. (2017) here: https://onlinelibrary.wiley.com/doi/abs/10.1111/1755-0998.12613.

Once we've visualized our data in terms of heterozygosity (H) and read ratio deviation (D), we will set limits for each parameter at the bottom of the chunk, and filter likely paralogs in chunk 4, once we convert our vcf file into a tidy format for downstream filtering. The goal with HDPlot is to capture the dense cloud of points that looks like a finger jutting out from the left side of the graph. Set the parameters H and D to try and capture the dense cloud of points as well as possible.

```{r}
popmap <- read_tsv(paste0(file_directory, popmap_file, ".txt"), col_names = FALSE) %>% rename(Indiv = X1, Pop = X2)

raw_vcf <- vcfR::read.vcfR(paste0(file_directory,file_prefix,".vcf")) #This takes a minute


#identify putative paralogs and plot - this may take a long time
paralog_calcs <- HDplot(raw_vcf) %>%
  arrange(D)

#Create heterozygosity vs read ratio deviation plot
paralog_calcs %>% ggplot()+geom_point(aes(x=H,y=D)) + 
  scale_y_continuous(name = "D", breaks = seq(-20, 20, by = 2)) +
  scale_x_continuous(name = "H", breaks = seq(0, 1, by = 0.05)) +
  ggtitle("Heterozygosity (H) vs Read Ratio Deviation (D)")

#Set limits for H and D based on plot from above
H_lim <- 0.6
D_lim <- 3.5


#Create heterozygosity vs read ratio plot, if wanted
#paralog_calcs %>% ggplot() + geom_point(aes(x=H,y=ratio))
```


##Chunk 3: Visualize genotype frequency and assess heterozygote miscall rate
The chunk below leverages Eric Anderson's package whoa to examine homozygosity bias: https://github.com/eriqande/whoa
We will calculate a heterozygote miscall rate, graph miscall rate by read depth, and then choose thresholds for filtering based on the graphs.
```{r}
#Make genotype frequency scatter plot
gfreqs <- exp_and_obs_geno_freqs(raw_vcf) #Calculate expected and observed genotype frequencies
geno_freqs_scatter(gfreqs)

#Infer heterozygote miscall rate - this can take a long time
het.miscall.all <- infer_m(raw_vcf, minBin = 1e15) #Set the minBin argument very high so it runs over all read depths

het.miscall.all$m_posteriors #mean column shows percent heterozygote miscall rate as a decimal (e.g. 0.1 = 10% miscall rate)

#Infer miscall rate for different read depth bins
#Can set the bin size to whatever we want; Consider targeting ~ 100 bins, so dividing n_total by 100 and setting the bin size to that (but can vary the bin size b to be whatever you want; see help for infer_m function for more info)
b <- het.miscall.all$m_posteriors$total_n/100
het.miscall.bins <- infer_m(raw_vcf, minBin = b)

#Plot heterozygote miscall rate at different read depths to help with filtering.
posteriors_plot(het.miscall.bins$m_posteriors)

#Based on plot above, set the threshold for filtering loci with a mean depth < mean.depth.threshold
min.mean.depth <- 25
max.mean.depth <- 125
```

## Chunk 4: Filter paralogs and convert vcf to tidy (long) format
We will now convert the vcf file into tidy format and simultaneously filter the SNPs HDplot identified as putative paralogs. The meta, fixed, and genotype regions of the vcf file are converted into separate components of a list, but we will just be working with the dataframes corresponding to the fixed and genotype components. The fixed dataframe is used for filtering loci, and the genotype dataframe is used for filtering samples.
```{r}
#Create dataframe of putative paralogs based on thresholds set above
paralogs <- paralog_calcs %>% filter(H > H_lim & abs(D) > D_lim) %>% 
  mutate(POS = as.integer(POS))

print(paste0("Identified ", nrow(paralogs), " likely paralogs. These will be removed from the dataset."))

##If interested in viewing or extracting specific pieces of information, use the below code.
#queryMETA(raw_vcf)
#queryMETA(raw_vcf, element = "DP") #Look at meta region. Can specify arguments that return more details

#info <- data.frame(getFIX(raw_vcf)) #Query the fixed region
#raw_vcf@gt[1:6, 1:4] #Look at gt portion of vcf file

#Meanings of the fields in the gt portion of the vcf file output from Stacks and VCFTools, which is colon delimited in each cell.
#meta_meanings <- cbind(c("GT", "DP", "AD", "GQ", "GL"), c("Genotype", "Read depth", "Allelic depth", "Genotype quality", "Genotype likelihood")) 


#Convert vcf file to tibble
options(tibble.print_max = 30)
raw_vcf_tidy <- vcfR2tidy(raw_vcf) #This takes awhile


#Extract fixed and gt dataframes and subset for useful columns
## ChromKey is a link between the fixed portion and the gt portion of the files. Need to merge the paralog df with the fixed df to grab the ChromKey column to filter paralogs at the next step.
paralogs <- paralogs %>% inner_join(raw_vcf_tidy$fix, by = c("CHROM", "POS")) %>% 
  dplyr::select(ChromKey, CHROM, POS)

#Convert fixed element to tidy format and remove paralogs
fix_tidy <- raw_vcf_tidy$fix %>% 
  anti_join(paralogs, by = c("CHROM", "POS"))

#Convert gt element to tidy format and remove paralogs 
gt_tidy <- raw_vcf_tidy$gt %>%
  left_join(popmap, by = "Indiv") %>% 
  anti_join(paralogs, by = c("ChromKey", "POS"))

#Store meta region
gt_meta <- raw_vcf_tidy$meta

#After extracting the relevant dataframes, remove the original imported vcf files from the environment to save working memory (you can always re-run parts of this chunk if you want to work directly with those files)
rm(raw_vcf)
rm(raw_vcf_tidy)
```


## Chunk 5: Set parameters for filtering and target values for visualization
There are three steps to the next chunk:
- Step 1: decide the minimum depth below which a locus will be classified as "missing' and then visualize the amount of missing data per individual.
- Step 2: based on the plot in the plot window, decide on the degree of data missingness that constitutes calling an individual as "failed". These individuals will be filtered before running any additional calculations. **This is an important step especially for smaller datasets where a number of samples didn't sequence well, because they will count towards the percentage of the total and can result in good loci being thrown out. In our experience, there is often a very obvious break between samples that worked and those that failed, which should help set this threshold as appropriate for the data**
- Step 3: set additional thresholds for filtering individuals and loci based on data missingness values calculated *after* removing failed individuals and low depth loci.
```{r}
## Step 1: set criteria below which very low-depth loci will be called as missing. Consider setting this to 2m
min_depth_indv <- 5 #Minimum depth for a locus to be included in subsequent analyses

#Recode loci in each individual as missing based on the threshold above
gt_tidy <- gt_tidy %>% mutate(gt_DP_recode = ifelse(gt_DP < min_depth_indv, NA, gt_DP))

#Calculate missingness per individual
all_indv <- gt_tidy %>% group_by(Indiv) %>% 
  summarize(percent_missing = sum(is.na(gt_DP_recode))/n() * 100) %>% 
  left_join(popmap, by = "Indiv")

#Plot missingness after recoding low-depth loci
p0.1 <- ggplot(all_indv, aes(x = percent_missing)) +
  geom_histogram(binwidth = 1, color = "black", fill = "grey95") +
  labs(x = "% missing data per indv")
p0.1 #look at this to gauge and tweak threshold for failed vs. passed samples

#Step 2: based on the plot, tweak the Ipass filter below. These individuals will be removed before running additional calculations (e.g. locus missingness). This should target individuals that clearly failed (didn't sequence well), to avoid skewing filtering below. Mostly this step is needed when datasets don't have a large total sample size and a decent number of the samples failed. If using, may want to play around with this value to see where appropriate threshold is (see histogram in plot window)
Ipass <- 70 


##Step 3: set criteria for additional filtering AFTER removing failed individuals and low-depth loci and recalculating missingness.
Imiss <- 30 #After removing failed individuals, filter individuals that still have this much missing data or more
Lmiss <- 10 #After removing failed individuals, remove loci that are missing in this percent of individuals
#minor_allele_indv <- 0 #Number of individuals the minor allele must be present in #JDS: remove? #LMK: I'd explain why this is here and keep, pointing out that (if I'm understanding how you used it below) if it's kept at 0 then it's redundant to removing non-variant "SNPs" below, but there may be reasons to set higher.

####------For viz only-------####
min_mean_depth_lim <- 10 #minimum mean depth per individual (used in vcftools)

#Set ideal targets. 
target_miss_per_indv <- Imiss #target for % missing loci per individual
target_miss_per_locus <- Lmiss #target for % missing individuals per individual locus
target_depth <- min.mean.depth #target depth per individual and per locus
```




## Chunk 6: Filter individuals and loci with missing data
Here, we use the parameters specified above to filter the dataset for missingness and read depth at two levels: individual and SNP. We create separate dataframes for the individuals/SNPs we wish to keep and those we will remove.
Then, we filter these SNPs/individuals from our main gt and fixed dataframes. 
Finally, we calculate the number of SNPs per locus after filtering.

**See the console after running this chunk for the summary of how many individuals/SNPs were kept and how many were removed.** If too many were removed or kept, simply change the parameters in the previous chunk and re-run.

**This chunk takes ~ a minute to run**
```{r}
#Remove failed individuals before filtering
gt_tidy_fltr <- remove_failed(gt_tidy) #Checked 05/23/2021

#Iteratively calculate locus missingness and make dataframe of loci to keep
#Set the "interval" argument to the interval that you would like to use to iteratively filter individuals. For instance, if interval = 5 (the default), then individuals with > 95% missing data will be removed first, then loci will be filtered, then individuals with 90% missing data, then loci, etc. 
#Because the interval for loci and individuals will not be the same (e.g. you want want to filter down to 30% missingness for individuals, but 10% for loci), the script will automatically calculate intervals for locus missingness, based on the number of intervals for such that for each iterative round of filtering individuals, there will be one corresponding round of filtering loci. If there are suggestions for how to do this better, I'm definitely open to them!
keep <- filter_all(gt_tidy_fltr, Imiss = Imiss, Lmiss = Lmiss, interval = 5, min.mean.depth = min.mean.depth, max.mean.depth = max.mean.depth) #loops over values of missingness from 100% to Imiss in intervals specified by the interval argument. Iterative filtering improves the number of individuals retained.

#Split output from above function into separate dataframes
fix_tidy_filt1 <- keep[[1]]
gt_tidy_filt1 <- keep[[2]]
(keep_indv <- keep[[3]] %>% arrange(percent_missing))
keep_loci <- keep[[4]]

#if curious, calculate individual and locus missingness, and how much we will remove if we filtered immediately without an iterative approach, just to compare. Will likely see fewer individuals and/or loci retained here than in the loop above.
#miss.stats <- calc_missing(gt_tidy_fltr) 

#filter gt dataframe so only good loci remain
gt_tidy_filt2 <- merge_tables(gt_tidy_filt1, fix_tidy_filt1)

#Calculate mean read depth per locus and individual after removing low depth loci
mean_read_depth_indv <- calc_depth_indv(gt_tidy_filt2)
mean_read_depth_loci <- calc_depth_loci(gt_tidy_filt2)

#Calculate number of SNPs per locus (reference-based)
SNPs_per_locus <- fix_tidy_filt1 %>% count(ChromKey, name = "SNPs")

#Show summary of filtering info in console
sumstats()

#Free up memory from unused objects
gc()
```



## Chunk 7: Plot filtered data
The plots produced here correspond to missingness and depth at the individual and population level after instituting the above filters.

The lines in the graphs correspond to: 
dark blue line = mean
light blue line = target
red line = threshold ie shouldn't be any above/below this point (depending on the metric)

**No required parameters for this chunk**
```{r}
plot_ind_stats_filt(keep_indv, keep_loci, mean_read_depth_indv, mean_read_depth_loci)

plot_pop_stats_filt(keep_indv, mean_read_depth_indv)

# plot # SNPs per locus
##This needs to be adapted for reference-based
SNPs_per_locus %>% 
  ggplot(aes(x = SNPs)) +
  geom_histogram(binwidth = 1, color = "black", fill = "grey95") + 
  labs(x = "number of SNPs per locus") +
  theme_standard
```



## Chunk 8: Evaluate heterozygosity, HWE, and LD

This chunk will calculate individual heterozygosity and produce a graph of individual level heterozygosity and a table showing how many SNPs conform (or don't) to hardy-weinberg equilibrium (HWE) based on the threshold we set for HWE_inquire at the top of the chunk.

**The main purpose of this chunk** is to help us visualize heterozygosity to see if we need to change any of the above filters. When the chunk is finished running, see the graph for plots of individual-level heterozygosity, and the console for the number of SNPs that conform/don't to HWE based on the user-defined threshold. If we don't have well-defined populations yet - ie if we are planning to use these data to identify putative subpopulations - then it's wise to set the threshold to a very low e.g. .000001 value. *Also reminder: any filtering for HWE should be omitted/adjusted for analyses targeting loci under selection*

See this site for more information about general filtering for HWE: https://thierrygosselin.github.io/radiator/reference/filter_hwe.html

**This chunk takes ~ a minute to run**
```{r}
#What are we calling our threshold for conformance to HWE?
HWE_inquire <- 0.000001

#Calculate individual heterozygosity
Obs_het_indv <- calc_het(gt_tidy_filt2, popmap = popmap)

#Vizualize heterozygosity
##Note these numbers may be quite different from the Fis values shown in the first plots because it is calculated differently here.
num_pops <- length(levels(factor(keep_indv$Pop)))
pop_cols2 <- brewer.pal(n = num_pops, name = "Dark2")

ggplot(Obs_het_indv, aes(x = factor(Pop), y = Obs_het, color = Pop)) + 
  geom_jitter() + 
  scale_color_manual(values = pop_cols2) + 
  labs(title = "Individual heterozygosity", x = "Population", y = "Heterozygosity") +
  #ylab(expression(paste(F[IT]))) +
  theme_facet

locus_HWE <- calc_hwe(gt_tidy_filt2, by_pop = T) #if by_pop = T, then HWE will be calculated for each population separately and return a dataframe of HWE values for each locus from each population with these values; if by_pop = F, then HWE will be calculated for the whole metapopulation

#Calculate LD -- JDS note to self: use genetics package; vcf tools takes too long (still running after four hours)


```

## Chunk 9: Filter for heterozygosity, HWE (i.e. putatively neutral SNPs) and LD
If desired, this chunk will filter our dataset for individual samples with high levels of heterozygosity, for individual loci that don't conform to HWE based on the thresholds we set at the top of the chunk, and/or for short-distance LD by randomly selecting one SNP per locus *after* the filter for HWE and heterozygosity. **If data were generated using a reference-based approach, then do not use the LD filter.**

If you'd prefer to skip filtering for HWE and LD, then change the "filter.HWE.LD" object to anything other than "yes". If you want to filter for one but not the other, play around with the function inside the if statement after the "create final filtered datasets" section. .. JDS note to self: flag this to streamline 07/14/2022.

```{r}
max_het <- 0.6
HWE_threshold <- HWE_inquire #HWE threshold for preserving a SNP
filter.HWE.LD <- "yes"

#Identify samples with very high heterozygosity
high_het_samples <- Obs_het_indv %>% filter(Obs_het > max_het)

#Mark individuals with good heterozygosity levels
keep_hetero_indiv <- gt_tidy_filt2 %>% anti_join(high_het_samples, by = "Indiv") %>%
  count(Indiv)

#Mark individuals with heterozygosity levels above the threshold
gt_tidy_hetero_remove <- gt_tidy_filt2 %>% semi_join(high_het_samples, by = "Indiv")

#Identify loci that are out of HWE, either within their population (if by_pop = T for the calc_hwe function) or over all populations (if by_pop = F)
(locus_HWE_remove <- locus_HWE %>% 
  filter(hwe_stats < HWE_threshold))

#Identify one random SNP per locus
##JDS note: Comment out below if reference-based approach was used upstream
 LD_loci_to_keep <- fix_tidy_filt1 %>% group_by(CHROM) %>%
  slice_sample(n=1)


####------Create final filtered dataframes------####
#Can edit below if wanting to only filter for LD or HWE
 if(filter.HWE.LD == "yes"){
  fix_tidy_filt_final <- fix_tidy_filt1 %>%
    semi_join(LD_loci_to_keep, by = c("ChromKey", "CHROM", "POS")) %>% #one SNP per locus
    anti_join(locus_HWE_remove, by = c("ChromKey", "POS")) #HWE; can include "pop" here if you just want to filter the loci from the population
} else {
  fix_tidy_filt_final <- fix_tidy_filt1
}


fix_tidy_4_final_merge <- fix_tidy_filt_final %>% dplyr::select(ChromKey, CHROM, POS, REF, ALT)

gt_tidy_filt_final <- gt_tidy_filt2 %>% 
  anti_join(gt_tidy_hetero_remove, by = "Indiv") %>% 
  inner_join(fix_tidy_4_final_merge, by = c("ChromKey", "POS")) #%>% 
  #dplyr::select(ChromKey, CHROM, POS, Indiv, gt_AD, gt_DP, gt_HQ, gt_GL, gt_GQ, gt_GT, gt_GT_alleles, allele_A, allele_B, zygosity, REF, ALT)#LMK: When the LD_loci_to_keep line is commented out, this line gives an error that ChromKey isn't found; may need to be adjusted for ref based/no short LD filtering option
  
final_sumstats()
```


## Chunk 10: Re-calculate missingness and heterozygosity after filtering
This chunk will calculate missingness and heterozygosity for the remaining dataset after filtering in the previous chunks. These calculations will be visualized in the following chunks.

```{r}
#Calculate missingness for remaining individuals
final_indv_stats <- gt_tidy_filt_final %>% group_by(Indiv) %>% 
  summarize(percent_missing = sum(is.na(gt_DP))/n() * 100) %>% 
  left_join(popmap, by = "Indiv")

#Calculate missingness for remaining loci
final_loc_stats <- gt_tidy_filt_final %>%
  group_by(ChromKey, POS) %>%
  summarize(percent_missing = sum(is.na(gt_DP))/n() * 100)

#Calculate mean read depth for remaining individuals using remaining loci
final_mean_depth_indv <- gt_tidy_filt_final %>% group_by(Indiv) %>% 
  summarize(mean_depth = mean(gt_DP, na.rm = TRUE)) %>% 
  arrange(desc(mean_depth)) %>% 
  left_join(popmap, by = "Indiv")

#Calculate mean read depth for remaining loci using remaining individuals
final_mean_depth_loc <- gt_tidy_filt_final %>% group_by(ChromKey, POS) %>% 
  summarize(mean_depth = mean(gt_DP, na.rm = TRUE)) %>% 
  ungroup() %>% 
  arrange(desc(mean_depth))

#Calculate SNPs per locus from remaining loci
##Only accurate for de novo approach
final_SNPs_per_locus <- fix_tidy_filt_final %>% count(ChromKey, name = "SNPs")

#Calculate final individual heterozygosity
final_obs_het_indv <- gt_tidy_filt_final %>%
  group_by(Indiv) %>% 
  summarize(Obs_het = sum(zygosity == "heterozygous", na.rm = TRUE)/sum(zygosity == "heterozygous" | zygosity == "homozygous", na.rm = TRUE)) %>% 
  left_join(popmap, by = "Indiv")
```


## Chunk 11: Make final plots after all filtering
Again, dark blue line = mean
light blue line = target
red line = threshold ie shouldn't be any above this point
```{r}
#Plot final individual stats
plot_final_indv(final_indv_stats, final_loc_stats, final_mean_depth_indv, final_mean_depth_loc)

#Plot final population stats
plot_final_pop(final_indv_stats, final_mean_depth_indv)

#Plot final heterozygosity and SNP stats
# If denovo approach was used upstream, run below
plot_final_het_denovo(final_SNPs_per_locus, final_obs_het_indv)

#If reference was used upstream, run below
#plot_final_het_reference(final_obs_het_indv)
```


## Chunk 12: Export data using radiator and vcfR
Radiator exports data to a variety of formats from a tidy dataframe. To take advantage of this, we need to reformat our final tidy dataframe to emulate radiator's expectations. Then, we call on radiator for export. 

All the specified file formats *except* genlight will produce both an external file in the working directory and save the file as an object in R for immediate analysis. The genlight format export doesn't work with radiator. Instead, we export a new vcf file using radiator, then read that in with vcfR and immediately export to genlight format using vcfR. The resulting genlight file is not created in the working directory, but *is* stored as an object in R.

The columns radiator expects (from running radiator::read_vcf) are:
1. GT_BIN (the dosage of ALT allele: 0, 1, 2 NA)
2. GT_VCF (the genotype coding VCFs: 0/0, 0/1, 1/1, ./.). GT_VCF column should have "0/0" for homozygous for ref allele, "0/1" for heterozygous, and "1/1" for homozygous for alternate allele
3. GT_VCF_NUC (the genotype coding in VCFs, but with nucleotides: A/C, ./.): FALSE
4. GT (the genotype coding 'a la genepop': 001002, 001001, 000000): FALSE; GT is 001001 for homozygous for the first (reference) allele, 001002 for heterozygous (001 for first allele, and 002 for second),  and 002002 for homozygous for second (alternate) allele

**This chunk takes a few minutes to run because the data need to be reformatted to fit radiator's expectations.**
```{r}
#Clear environment of large files that aren't being used anymore so the rest of the chunk can run faster
#gt_tidy <- NULL
rm(gt_tidy_filt1)
#fix_tidy <- NULL
rm(fix_tidy_filt1)

####---------Format final data to conform to radiator's expectations for export---------####

#Rename columns for popmap so it can be integrated into radiator
popmap2 <- popmap %>% rename(INDIVIDUALS = Indiv, STRATA = Pop)

#DE NOVO -- NOTE: if using a reference based approach, may need to edit the CHROM column so that the values in that column are numeric rather than alpha-numeric. 
#An example of how to do it is below

# Example: If using a reference-based approach where the reference includes "QVIC" and ".1" as part of the value in the CHROM column, do this:
# CHROM_num <- gt_tidy_filt_final %>% pull(CHROM) %>% 
#   stringr::str_remove(pattern = "QVIC") %>% 
#   stringr::str_remove(pattern = "\\.1")
# 
# gt_tidy_filt_final2 <- gt_tidy_filt_final %>% dplyr::select(-CHROM)
# gt_tidy_filt_final2$CHROM <- as.numeric(CHROM_num)

##If having issues exporting after using a reference-based approach, feel free to reach out and we can brainstorm together: jswenson@umass.edu

#Assuming the above is not applicable ... 

#Re-format final filtered dataframe for export with radiator
radiator_temp <- gt_tidy_filt_final %>% 
  rename(INDIVIDUALS = Indiv,
         GT_VCF = gt_GT,
         GT_VCF_NUC = gt_GT_alleles) %>% 
  mutate(LOCUS = as.numeric(CHROM)) %>% 
  mutate(MARKERS = paste0("1__",LOCUS,"__",POS),
         CHROM = as.character(1),
         COL = LOCUS - 1,
         GT = ifelse(GT_VCF == "0/0", "001001",
                     ifelse(GT_VCF == "0/1", "001002", 
                            ifelse(GT_VCF == "1/1", "002002", "000000"))),
         GT_BIN = ifelse(GT_VCF == "0/0", 0,
                         ifelse(GT_VCF == "0/1", 1, 2)),
         GT_VCF = replace_na(GT_VCF, "./."),
         GT_VCF_NUC = replace_na(GT_VCF_NUC, "./.")) %>% 
  mutate(LOCUS = as.character(LOCUS),
         POS = as.character(POS),
         GT_BIN = as.integer(GT_BIN)) %>% 
  inner_join(popmap2, by = "INDIVIDUALS") %>% 
  dplyr::arrange(MARKERS)

#Add column for variant ID
radiator_temp$VARIANT_ID <- radiator_temp %>% group_by(MARKERS) %>% 
  group_indices(MARKERS)

#Re-arrange columns
radiator_temp <- radiator_temp %>% dplyr::select(VARIANT_ID, MARKERS, CHROM, LOCUS, POS, COL, REF, ALT, INDIVIDUALS, GT_BIN, GT, GT_VCF, gt_AD, gt_DP, gt_GQ, STRATA)

#Write final tidy dataframe so we can import it later without having to re-run the whole script
write_csv2(radiator_temp, file = "radiator_filtered_tibble.csv")

####-----Export to other formats------####
#Export to vcf
radiator::write_vcf(radiator_temp, filename = "radiator_ref_filtered_vcf_05.24.2021")

#export to genind
genind <- radiator::write_genind(radiator_temp, write = TRUE) #radiator -- WORKS

#export to genlight using vcfR 
#have to read in vcf file exported above to vcfR object, then use vcfR to export. The file stays within R
vcf_int <- vcfR::read.vcfR("radiator_ref_filtered_vcf_05.24.2021.vcf")
genlight <- vcfR2genlight(vcf_int) #vcfR

#Remove intermediate vcf file from environment
vcf_int <- NULL

#plink for Plink
plink <- radiator::write_plink(radiator_temp, filename = "CNR_filtered_plink_minallele_0.05") #radiator -- WORKS

#genepop for Genepop
genepop <- radiator::write_genepop(radiator_temp, filename = "CNR_filtered_genepop_minallele_0.05") #radiator -- WORKS

#gtypes for strataG
gtypes <- radiator::write_gtypes(radiator_temp, write = TRUE) #radiator -- WORKS
```